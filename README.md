# ğŸ§  Informe del Taller de Aprendizaje AutomÃ¡tico No Supervisado y Supervisado â€“ Dataset de Setas

## 1ï¸âƒ£ DescripciÃ³n General

El presente trabajo tiene como objetivo aplicar tÃ©cnicas de **aprendizaje automÃ¡tico supervisado y no supervisado** sobre el dataset de **setas (Mushrooms)** del repositorio **UCI Machine Learning Repository**.  
Este conjunto de datos contiene informaciÃ³n sobre diferentes caracterÃ­sticas fÃ­sicas y quÃ­micas de los hongos, con la finalidad de determinar si son **comestibles (edible)** o **venenosos (poisonous)**.

El proyecto combina dos enfoques complementarios:

- **Aprendizaje no supervisado:** PCA y K-Means, para explorar la estructura interna de los datos y descubrir posibles patrones.
- **Aprendizaje supervisado:** Random Forest, para construir un modelo predictivo capaz de clasificar correctamente los hongos segÃºn sus caracterÃ­sticas.

---

## 2ï¸âƒ£ Flujo de Trabajo del Proyecto

El desarrollo del proyecto siguiÃ³ una secuencia lÃ³gica y estructurada conformada por las siguientes etapas:

1. **Carga y exploraciÃ³n del dataset:**  
   Se revisaron los valores nulos, el balance entre clases y la estructura de los datos.

2. **Preprocesamiento:**  
   - Se codificaron las variables categÃ³ricas mediante *One-Hot Encoding*.  
   - Se normalizaron los valores con *StandardScaler* para optimizar el rendimiento de los modelos.  
   - Se generÃ³ un dataset limpio y preparado para el modelado.

3. **AnÃ¡lisis de Componentes Principales (PCA):**  
   Se redujo la dimensionalidad a dos componentes para visualizar la estructura general de los datos en un plano 2D.

4. **Clustering con K-Means:**  
   Se aplicÃ³ para identificar agrupamientos naturales sin usar etiquetas reales, comparando los clusters con las clases verdaderas.

5. **ClasificaciÃ³n Supervisada con Random Forest:**  
   Se entrenÃ³ un modelo para predecir si un hongo es comestible o venenoso, evaluando mÃ©tricas de rendimiento y overfitting.

6. **EvaluaciÃ³n Final:**  
   Se analizaron los resultados de cada tÃ©cnica y se extrajeron conclusiones generales del comportamiento de los datos.

---

## 3ï¸âƒ£ Resultados Principales

### ğŸ”¹ PCA â€“ AnÃ¡lisis de Componentes Principales

El PCA permitiÃ³ reducir la dimensionalidad del dataset a 2 componentes principales, capturando la mayor parte de la varianza.  
Sin embargo, los puntos (hongos comestibles y venenosos) se mostraban mezclados, sin una frontera clara entre ambos grupos.  

ğŸ“˜ **InterpretaciÃ³n:**  
El PCA no clasifica, solo resume la informaciÃ³n y proyecta los datos en un espacio reducido para facilitar su visualizaciÃ³n.  
Fue Ãºtil para observar la estructura general antes de aplicar K-Means.

---

### ğŸ”¹ K-Means â€“ Agrupamiento No Supervisado

El **mÃ©todo del codo** indicÃ³ que el nÃºmero Ã³ptimo de clusters se encuentra entre **K = 2 y K = 3**, coincidiendo con las dos clases principales del dataset.

Con **K = 2**, el algoritmo separÃ³ los datos en dos grupos visualmente distinguibles (puntos amarillos y morados).  

ğŸ“Š **MÃ©tricas de agrupamiento:**

| MÃ©trica | Valor | InterpretaciÃ³n |
|----------|--------|----------------|
| Adjusted Rand Index (ARI) | 0.6182 | Buena concordancia entre clusters y clases reales |
| Normalized Mutual Information (NMI) | 0.5707 | CorrelaciÃ³n moderada entre los agrupamientos y las etiquetas verdaderas |

ğŸ’¡ **ConclusiÃ³n PCA vs K-Means:**  
La diferencia entre ambas visualizaciones no proviene de un cambio en los datos, sino en cÃ³mo se interpretan y agrupan.  
Mientras **PCA** solo reduce dimensiones, **K-Means** detecta patrones de similitud y logra distinguir grupos mÃ¡s evidentes en ese mismo espacio reducido.

âœ… **InterpretaciÃ³n complementaria:**  
Estos valores estÃ¡n por encima de **0.5**, lo que indica una **buena concordancia** entre los grupos creados por K-Means y las etiquetas verdaderas (comestible vs venenoso).  
El **grÃ¡fico del codo** te dice **cuÃ¡ntos clusters formar**,  
y los valores de **ARI y NMI** te confirman **quÃ© tan bien se formaron esos clusters**.  
En este anÃ¡lisis, ambos indicadores muestran que **K-Means logrÃ³ una separaciÃ³n bastante buena** entre hongos comestibles y venenosos.

---

## 4ï¸âƒ£ ClasificaciÃ³n Supervisada â€“ Random Forest

El modelo Random Forest se entrenÃ³ con una divisiÃ³n del 70% de los datos para entrenamiento y 30% para prueba.

### ğŸ“ˆ MÃ©tricas de Rendimiento

| MÃ©trica | Valor |
|----------|--------|
| Accuracy global | 1.0000 |
| Precision (ambas clases) | 1.00 |
| Recall (ambas clases) | 1.00 |
| F1-score | 1.00 |

ğŸ“Š **Matriz de ConfusiÃ³n:**

[[1263 0]
[ 0 1175]]

yaml
Copiar cÃ³digo

âœ… El modelo clasificÃ³ correctamente todas las observaciones, sin cometer errores.

ğŸ“˜ **InterpretaciÃ³n:**  
El modelo alcanzÃ³ una precisiÃ³n perfecta (100%), demostrando una excelente capacidad para distinguir hongos comestibles de venenosos.

---

## 5ï¸âƒ£ EvaluaciÃ³n del Overfitting

| Tipo de Accuracy | Valor |
|-------------------|--------|
| Entrenamiento | 1.0000 |
| Prueba | 1.0000 |
| Overfitting (train - test) | 0.0000 |

âœ… **ConclusiÃ³n:**  
No se detecta overfitting significativo.  
El modelo generaliza correctamente, manteniendo el mismo rendimiento en entrenamiento y prueba.  
Esto significa que **no memorizÃ³ los datos**, sino que aprendiÃ³ los **patrones reales** del dataset.

---

## 6ï¸âƒ£ Conclusiones Generales

- El dataset fue correctamente preprocesado: sin valores nulos, balanceado y con variables bien codificadas.  
- **PCA** revelÃ³ que los datos no son fÃ¡cilmente separables visualmente.  
- **K-Means** confirmÃ³ la existencia de **dos grupos naturales**, alineados con las clases comestible y venenosa.  
- **Random Forest** logrÃ³ una **clasificaciÃ³n perfecta (100%)**, sin errores ni sobreajuste.  
- **ARI y NMI** demostraron que los clusters descubiertos tienen una buena relaciÃ³n con las etiquetas verdaderas.  

ğŸ“Š En conjunto, los resultados evidencian que las caracterÃ­sticas del dataset contienen **informaciÃ³n suficiente y representativa** para distinguir ambas clases con total precisiÃ³n.

---

## 7ï¸âƒ£ ReflexiÃ³n Final

Este trabajo demuestra cÃ³mo el uso combinado de **tÃ©cnicas no supervisadas (PCA, K-Means)** y **supervisadas (Random Forest)** permite obtener una comprensiÃ³n profunda de los datos:  
desde su estructura interna hasta su comportamiento predictivo.

El modelo desarrollado es **robusto, interpretable y eficiente**, representando una soluciÃ³n Ã³ptima para  la clasificaciÃ³n de hongos segÃºn sus propiedades fÃ­sicas y quÃ­micas.

### ğŸ”„ Flujo de Trabajo del Proyecto

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     1ï¸âƒ£ Carga del Dataset   â”‚
â”‚ (UCI Machine Learning Repo)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     2ï¸âƒ£ ExploraciÃ³n de Datos â”‚
â”‚ - VerificaciÃ³n de nulos     â”‚
â”‚ - Balance de clases         â”‚
â”‚ - Variables categÃ³ricas     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     3ï¸âƒ£ Preprocesamiento     â”‚
â”‚ - One-Hot Encoding          â”‚
â”‚ - Escalado (StandardScaler) â”‚
â”‚ - ExportaciÃ³n CSV limpio    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       4ï¸âƒ£ PCA (2D)           â”‚
â”‚ - ReducciÃ³n de dimensional. â”‚
â”‚ - VisualizaciÃ³n de clases   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     5ï¸âƒ£ K-Means Clustering  â”‚
â”‚ - MÃ©todo del codo (K Ã³ptimo)â”‚
â”‚ - ARI / NMI                 â”‚
â”‚ - VisualizaciÃ³n 2D grupos   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6ï¸âƒ£ Random Forest (RF)     â”‚
â”‚ - Entrenamiento/Test split â”‚
â”‚ - Accuracy y OOB Score     â”‚
â”‚ - Overfitting (0.00)       â”‚
â”‚ - Importancia variables    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    7ï¸âƒ£ InterpretaciÃ³n Final  â”‚
â”‚ - PCA vs K-Means conclusiÃ³nâ”‚
â”‚ - Modelo RF sin sobreajusteâ”‚
â”‚ - Insights clave del datasetâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

